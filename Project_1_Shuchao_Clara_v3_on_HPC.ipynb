{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "• Implement parallel KNN for regression (with the tools we already reviewed).\n",
    "• We will provide the sequential code.\n",
    "• You will implement a parallel version of the algorithm.\n",
    "• The parallel version should give the same results (output) as the sequential version.\n",
    "• You will do a benchmark (at least 30 runs) and present the result of real time and also CPU time with average and standard deviation.\n",
    "• Main things to highlight:\n",
    "• Approach you used to parallelize and why.\n",
    "• The speed-up gain in real time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "• Teams of max 3 people.\n",
    "• Send us the code (or jupyter notebook, an easy way to run it).\n",
    "• If its .py files put it in a zip folder.\n",
    "• Put your name (or the team members name) on the file or the notebook.\n",
    "• Explain your logic with code comments.\n",
    "• Use the HPC of the university (Aion).\n",
    "• Do not change the random seed of the code to replicate the results.\n",
    "• Send the project results before 12 of November (sending later its possible but will remove 30% of the total score)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T15:13:25.749800900Z",
     "start_time": "2024-11-03T15:13:25.660192800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from KNNClassifier import KNNClassifier\n",
    "import numpy as np\n",
    "from joblib import Parallel, delayed\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T15:13:27.893352900Z",
     "start_time": "2024-11-03T15:13:27.711127800Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape (100000, 500) - y_train shape (100000,)\n"
     ]
    }
   ],
   "source": [
    "# Example with random data\n",
    "rows = 100000\n",
    "cols = 500\n",
    "np.random.seed(699)\n",
    "X_train = np.random.rand(rows * cols).reshape((rows, cols))\n",
    "y_train = np.random.randint(2, size=rows)\n",
    "print(f'X_train shape {X_train.shape} - y_train shape {y_train.shape}')\n",
    "\n",
    "# Create random indices to test\n",
    "test_size = 1000\n",
    "X_test = np.random.randint(rows, size=test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The version without parallel\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Sample data\n",
    "rows = 100000\n",
    "cols = 500\n",
    "np.random.seed(699)\n",
    "X_train = np.random.rand(rows, cols)\n",
    "y_train = np.random.randint(2, size=rows)\n",
    "\n",
    "# Sample test indices\n",
    "test_size = 1000\n",
    "X_test_indices = np.random.randint(rows, size=test_size)\n",
    "\n",
    "# Modify the KNN classifier to handle smaller batches\n",
    "class KNNClassifier:\n",
    "    def __init__(self, k=3):\n",
    "        self.k = k\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "\n",
    "    def euclidean_distance(self, x1, x2):\n",
    "        return np.sqrt(np.sum((x1 - x2) ** 2))\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Process predictions in batches to reduce memory usage\n",
    "        batch_size = 50  # Define a manageable batch size\n",
    "        y_pred = []\n",
    "        for i in range(0, len(X), batch_size):\n",
    "            X_batch = X[i:i+batch_size]\n",
    "            batch_pred = [self._predict(x) for x in X_batch]\n",
    "            y_pred.extend(batch_pred)\n",
    "            # Print progress within each batch\n",
    "            print(f\"Processed batch {i // batch_size + 1}/{len(X) // batch_size + 1}\")\n",
    "        return np.array(y_pred)\n",
    "\n",
    "    def _predict(self, x):\n",
    "        # Calculate distances from the input point to all training points\n",
    "        distances = [self.euclidean_distance(x, x_train) for x_train in self.X_train]\n",
    "        # Sort by distance and get indices of the first k neighbors\n",
    "        k_indices = np.argsort(distances)[:self.k]\n",
    "        # Get the labels of the k nearest neighbors\n",
    "        k_nearest_labels = [self.y_train[i] for i in k_indices]\n",
    "        # Return the most common label\n",
    "        return np.bincount(k_nearest_labels).argmax()\n",
    "\n",
    "# Create an instance of the KNN classifier\n",
    "knn = KNNClassifier(k=2)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Perform multiple runs with smaller prediction batches\n",
    "correct_predictions = []\n",
    "times = []\n",
    "cpu_times = []\n",
    "\n",
    "for i in range(30):\n",
    "    print(f\"\\nStarting run {i+1}...\")\n",
    "    start_time = time.time()\n",
    "    start_cpu = time.process_time()\n",
    "\n",
    "    # Split X_test_indices into smaller chunks\n",
    "    chunk_size = 200  # Define chunk size to limit memory usage\n",
    "    correct_count = 0\n",
    "\n",
    "    for j in range(0, test_size, chunk_size):\n",
    "        # Get a chunk of test indices\n",
    "        chunk_indices = X_test_indices[j:j+chunk_size]\n",
    "        X_test_chunk = X_train[chunk_indices]\n",
    "\n",
    "        # Make predictions on the chunk\n",
    "        predictions = knn.predict(X_test_chunk)\n",
    "        correct_count += np.sum(y_train[chunk_indices] == predictions)  # Correct predictions in this chunk\n",
    "\n",
    "        # Print progress within each chunk\n",
    "        print(f\"Chunk {j // chunk_size + 1}/{test_size // chunk_size + 1} completed in Run {i+1}\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    end_cpu = time.process_time()\n",
    "    elapsed_cpu_time = end_cpu - start_cpu\n",
    "    elapsed_time = end_time - start_time\n",
    "    times.append(elapsed_time)\n",
    "    cpu_times.append(elapsed_cpu_time)\n",
    "\n",
    "    # Record and print results for this run\n",
    "    correct_predictions.append(correct_count)\n",
    "    print(f\"Run {i+1}: Correct Predictions = {correct_count}, Time = {elapsed_time:.4f} seconds, Time (CPU) = {elapsed_cpu_time:.4f} seconds\")\n",
    "\n",
    "# Calculate total times, averages and standard deviations\n",
    "average_correct = np.mean(correct_predictions)\n",
    "average_time = np.mean(times)\n",
    "average_cpu_time = np.mean(cpu_times)\n",
    "sd_time = np.std(times)\n",
    "sd_cpu_time = np.std(cpu_times)\n",
    "total_time = np.sum(times)\n",
    "total_cpu_time = np.sum(cpu_times)\n",
    "print(f\"\\nAverage Correct Predictions over 30 runs: {average_correct}\")\n",
    "print(f\"Total Execution Time over 30 runs: {total_time:.4f} seconds\")\n",
    "print(f\"Average Running Time over 30 runs: {average_time:.4f} seconds\")\n",
    "print(f\"Average CPU Running Time over 30 runs: {average_cpu_time:.4f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T15:40:59.275486Z",
     "start_time": "2024-11-03T15:37:51.091030300Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1: Correct Predictions = 743, Time = 120.4701 seconds\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 52\u001b[0m\n\u001b[0;32m     47\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# Make predictions using the test samples\u001b[39;00m\n\u001b[1;32m---> 52\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mknn_p\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m correct_count \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(y_train[X_test] \u001b[38;5;241m==\u001b[39m predictions)  \u001b[38;5;66;03m# Count correct predictions\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# Update total correct predictions\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[5], line 21\u001b[0m, in \u001b[0;36mKNNClassifier_p.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;66;03m# Predict labels for the provided data using parallel processing\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predict\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Use all available cores\u001b[39;00m\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(y_pred)\n",
      "File \u001b[1;32mD:\\软件\\Python\\Lib\\site-packages\\joblib\\parallel.py:1952\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1946\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   1947\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   1948\u001b[0m \u001b[38;5;66;03m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[0;32m   1949\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   1950\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1952\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\软件\\Python\\Lib\\site-packages\\joblib\\parallel.py:1595\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1592\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1594\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1595\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1597\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1598\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1599\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1600\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1601\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mD:\\软件\\Python\\Lib\\site-packages\\joblib\\parallel.py:1707\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1702\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1703\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1704\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1705\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[0;32m   1706\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1707\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m   1708\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1710\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1711\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1712\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# The version with parallel\n",
    "\n",
    "# Define the KNN classifier class\n",
    "class KNNClassifier_p:\n",
    "    def __init__(self, k=3):\n",
    "        self.k = k  # Number of neighbors\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.X_train = X  # Store the training data\n",
    "        self.y_train = y  # Store the training labels\n",
    "\n",
    "    def euclidean_distance(self, x1, x2):\n",
    "        # Calculate the Euclidean distance between two points\n",
    "        diff = (x1 - x2)\n",
    "        sqr_diff = diff ** 2\n",
    "        sqr_diff_sum = np.sum(sqr_diff)\n",
    "        return np.sqrt(sqr_diff_sum)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Predict labels for the provided data using parallel processing\n",
    "        y_pred = Parallel(n_jobs=-1)(delayed(self._predict)(x) for x in X)  # Use all available cores\n",
    "        return np.array(y_pred)\n",
    "\n",
    "    def _predict(self, x):\n",
    "        # Calculate distances from the input point to all training points\n",
    "        distances = [self.euclidean_distance(x, x_train) for x_train in self.X_train]\n",
    "        # Sort distances and return indices of the first k neighbors\n",
    "        k_indices = np.argsort(distances)[:self.k]\n",
    "        # Extract the labels of the k nearest neighbor training samples\n",
    "        k_nearest_labels = [self.y_train[i] for i in k_indices]\n",
    "        # Return the most common class label among the k nearest neighbors\n",
    "        most_common = np.bincount(k_nearest_labels).argmax()\n",
    "        return most_common\n",
    "\n",
    "# Create an instance of the KNN classifier\n",
    "knn_p = KNNClassifier_p(k=2)\n",
    "# Fit the model\n",
    "knn_p.fit(X_train, y_train)\n",
    "\n",
    "# Initialize variables to record total correct predictions and total time\n",
    "total_correct_predictions = 0\n",
    "correct_predictions = []\n",
    "times = []\n",
    "cpu_times = []\n",
    "\n",
    "# Perform 30 runs of training and prediction\n",
    "for i in range(30):\n",
    "    # Record start time\n",
    "    start_time = time.time()\n",
    "    start_cpu = time.process_time()\n",
    "\n",
    "    # Make predictions using the test samples\n",
    "    predictions = knn_p.predict(X_train[X_test])\n",
    "    correct_count = np.sum(y_train[X_test] == predictions)  # Count correct predictions\n",
    "\n",
    "    # Update total correct predictions\n",
    "    total_correct_predictions += correct_count\n",
    "\n",
    "    # Record end time\n",
    "    end_time = time.time()\n",
    "    end_cpu = time.process_time()\n",
    "    elapsed_cpu_time = end_cpu - start_cpu\n",
    "    elapsed_time = end_time - start_time\n",
    "    times.append(elapsed_time)\n",
    "    cpu_times.append(elapsed_cpu_time)\n",
    "\n",
    "    # Record and print results for this run\n",
    "    correct_predictions.append(correct_count)\n",
    "    print(f\"Run {i+1}: Correct Predictions = {correct_count}, Time = {elapsed_time:.4f} seconds, Time (CPU) = {elapsed_cpu_time:.4f} seconds\")\n",
    "\n",
    "    # Output the results for this run\n",
    "    print(f'Run {i+1}: Correct Predictions = {correct_count}, Time = {elapsed_time:.4f} seconds')\n",
    "\n",
    "# Calculate total times, averages and standard deviations\n",
    "average_correct = total_correct_predictions / 30\n",
    "average_time = np.mean(times)\n",
    "average_cpu_time = np.mean(cpu_times)\n",
    "sd_time = np.std(times)\n",
    "sd_cpu_time = np.std(cpu_times)\n",
    "total_time = np.sum(times)\n",
    "total_cpu_time = np.sum(cpu_times)\n",
    "print(f\"\\nAverage Correct Predictions over 30 runs: {average_correct}\")\n",
    "print(f\"Total Execution Time over 30 runs: {total_time:.4f} seconds\")\n",
    "print(f\"Average Running Time over 30 runs: {average_time:.4f} seconds\")\n",
    "print(f\"Average CPU Running Time over 30 runs: {average_cpu_time:.4f} seconds\")\n",
    "print(f\"Standard Deviation of Running Time over 30 runs: {sd_time:.4f} seconds\")\n",
    "print(f\"Standard Deviation of CPU Running Time over 30 runs: {sd_cpu_time:.4f} seconds\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
